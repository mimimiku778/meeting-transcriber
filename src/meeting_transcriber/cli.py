#!/usr/bin/env python3
"""CLI tool for meeting transcription."""

# Suppress all warnings before any imports
import warnings
warnings.filterwarnings("ignore")

import argparse
import subprocess
import sys
from pathlib import Path

from .transcriber import transcribe_video, format_timestamp
from .diarization import load_diarization_pipeline, diarize_audio, assign_speakers_to_segments

LOG_FILE = Path("/tmp/meeting-transcriber.log")


def watch_progress():
    """Watch MCP server log with tail -f."""
    print("ğŸ“¡ ãƒ­ã‚°ã‚’ç›£è¦–ä¸­... (Ctrl+C ã§çµ‚äº†)")
    print()
    try:
        subprocess.run(["tail", "-f", str(LOG_FILE)])
    except KeyboardInterrupt:
        pass


def main():
    parser = argparse.ArgumentParser(
        description="ä¼šè­°å‹•ç”»ã‹ã‚‰è©±è€…è­˜åˆ¥ä»˜ãæ–‡å­—èµ·ã“ã—ã‚’ç”Ÿæˆ"
    )
    parser.add_argument(
        "video_path",
        nargs="?",
        help="å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹"
    )
    parser.add_argument(
        "--watch", "-w",
        action="store_true",
        help="MCPã‚µãƒ¼ãƒãƒ¼ã®é€²è¡ŒçŠ¶æ³ã‚’ç›£è¦–"
    )
    parser.add_argument(
        "-o", "--output",
        help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆçœç•¥æ™‚ã¯å‹•ç”»ã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã« _transcript.txtï¼‰"
    )
    parser.add_argument(
        "-m", "--model",
        default="medium",
        choices=["small", "medium", "large", "large-v3", "turbo"],
        help="Whisperãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º (default: medium)"
    )
    parser.add_argument(
        "--fast",
        action="store_true",
        help="é€Ÿåº¦å„ªå…ˆãƒ¢ãƒ¼ãƒ‰ï¼ˆç²¾åº¦è¨­å®šã‚’ç·©å’Œï¼‰"
    )
    parser.add_argument(
        "--no-diarization",
        action="store_true",
        help="è©±è€…è­˜åˆ¥ã‚’ã‚¹ã‚­ãƒƒãƒ—"
    )
    parser.add_argument(
        "--speakers",
        type=int,
        default=None,
        help="è©±è€…æ•°ã‚’æŒ‡å®šï¼ˆç²¾åº¦å‘ä¸Šï¼‰"
    )

    args = parser.parse_args()

    # Watch mode
    if args.watch:
        watch_progress()
        return

    # Normal transcription mode requires video_path
    if not args.video_path:
        parser.error("å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ï¼ˆã¾ãŸã¯ --watch ã§é€²è¡ŒçŠ¶æ³ã‚’ç›£è¦–ï¼‰")

    video_path = Path(args.video_path).resolve()
    if not video_path.exists():
        print(f"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {video_path}", file=sys.stderr)
        sys.exit(1)

    output_path = args.output
    if output_path is None:
        output_path = video_path.parent / f"{video_path.stem}_transcript.txt"
    else:
        output_path = Path(output_path)

    max_accuracy = not args.fast
    mode = "é€Ÿåº¦å„ªå…ˆ" if args.fast else "æœ€é«˜ç²¾åº¦"

    print(f"å‹•ç”»: {video_path}")
    print(f"å‡ºåŠ›: {output_path}")
    print(f"ãƒ¢ãƒ‡ãƒ«: {args.model} ({mode})")
    print()

    # Step 1: Transcribe
    print("1/3 éŸ³å£°æŠ½å‡ºãƒ»æ–‡å­—èµ·ã“ã—ä¸­...")
    whisper_result, audio_path = transcribe_video(str(video_path), args.model, max_accuracy)
    print(f"    å®Œäº† ({len(whisper_result.get('segments', []))} ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ)")

    # Step 2: Speaker diarization
    if args.no_diarization:
        print("2/3 è©±è€…è­˜åˆ¥: ã‚¹ã‚­ãƒƒãƒ—")
        segments_with_speakers = [
            {
                "start": seg["start"],
                "end": seg["end"],
                "text": seg["text"].strip(),
                "speaker": "ç™ºè©±è€…",
            }
            for seg in whisper_result.get("segments", [])
        ]
    else:
        print("2/3 è©±è€…è­˜åˆ¥ä¸­...")
        pipeline = load_diarization_pipeline()
        diarization_segments = diarize_audio(audio_path, pipeline, num_speakers=args.speakers)
        segments_with_speakers = assign_speakers_to_segments(whisper_result, diarization_segments)
        unique_speakers = sorted(set(seg["speaker"] for seg in segments_with_speakers if seg["speaker"] != "ä¸æ˜"))
        print(f"    å®Œäº† (è©±è€…: {', '.join(unique_speakers)})")

    # Step 3: Format and save
    print("3/3 ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆä¸­...")
    output_lines = []
    current_speaker = None
    current_text_parts = []
    current_start = None

    for segment in segments_with_speakers:
        if segment["speaker"] != current_speaker:
            if current_speaker is not None and current_text_parts:
                timestamp = format_timestamp(current_start)
                text = "".join(current_text_parts).strip()
                output_lines.append(f"{current_speaker} ({timestamp})")
                output_lines.append(text)
                output_lines.append("")

            current_speaker = segment["speaker"]
            current_text_parts = [segment["text"]]
            current_start = segment["start"]
        else:
            current_text_parts.append(segment["text"])

    if current_speaker is not None and current_text_parts:
        timestamp = format_timestamp(current_start)
        text = "".join(current_text_parts).strip()
        output_lines.append(f"{current_speaker} ({timestamp})")
        output_lines.append(text)
        output_lines.append("")

    output_text = "\n".join(output_lines)
    output_path.write_text(output_text, encoding="utf-8")

    print(f"    å®Œäº†")
    print()
    print(f"å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_path}")


if __name__ == "__main__":
    main()
